# Min-Char-RNN
min_char_rnn in python

Summary

RNNs allow a lot of flexibility in architecture design
-
Vanilla RNNs are simple but donâ€™t work very well
-
Common to use LSTM or GRU: their additive interactions 
improve gradient flow
-
Backward flow of gradients in RNN can explode or vanish. 
Exploding is controlled with gradient clipping. Vanishing is 
controlled with additive interactions (LSTM)
-
Better/simpler architectures are a hot topic of current research
-
Better understanding (both theoretical and empirical) is needed.
